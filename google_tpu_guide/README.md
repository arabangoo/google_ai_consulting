# Google TPU ì™„ë²½ ê°€ì´ë“œ

[![TPU](https://img.shields.io/badge/Google-TPU-4285F4?logo=google&logoColor=white)](https://cloud.google.com/tpu)
[![JAX](https://img.shields.io/badge/JAX-Supported-orange)](https://jax.readthedocs.io/)
[![PyTorch/XLA](https://img.shields.io/badge/PyTorch%2FXLA-Supported-EE4C2C?logo=pytorch&logoColor=white)](https://pytorch.org/xla/)

ì´ ê°€ì´ë“œëŠ” Google TPUë¥¼ ì²˜ìŒ ë‹¤ë£¨ëŠ” ì‚¬ìš©ìë„ ë°”ë¡œ í•™ìŠµ ë° ì„œë¹„ìŠ¤ ê°œë°œì„ ì‹œì‘í•  ìˆ˜ ìˆë„ë¡ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.

> **ì‹¤ì „ ê²½í—˜ ê¸°ë°˜ ê°€ì´ë“œ**: ì´ ë¬¸ì„œëŠ” ì‹¤ì œ TPU í™˜ê²½ ìš´ì˜ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. Google Colab, Kaggle ë“± í´ë¼ìš°ë“œ TPU í™˜ê²½ì—ì„œ ëŒ€ê·œëª¨ ëª¨ë¸ í•™ìŠµë¶€í„° ì¼ìƒì ì¸ ìš´ì˜ê¹Œì§€ ëª¨ë“  ê³¼ì •ì„ ë‹¤ë£¹ë‹ˆë‹¤.

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 30ì´ˆ ë§Œì— TPU ì‹œì‘í•˜ê¸°

```python
# Google Colabì—ì„œ ë°”ë¡œ ì‹¤í–‰
!pip install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html

import torch
import torch_xla.core.xla_model as xm

# TPU ë””ë°”ì´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
device = xm.xla_device()
print(f"Using device: {device}")  # xla:0

# TPUì—ì„œ í…ì„œ ì—°ì‚°
x = torch.randn(1000, 1000).to(device)
y = torch.matmul(x, x)
print("âœ“ TPU ì—°ì‚° ì„±ê³µ!")
```

### ì™œ ì´ ê°€ì´ë“œì¸ê°€?

- âœ… **ì‹¤ì „ ì¤‘ì‹¬**: ì´ë¡ ì´ ì•„ë‹Œ ì‹¤ì œ ì½”ë“œì™€ ê²½í—˜ ê³µìœ 
- âœ… **ì™„ì „í•œ ì˜ˆì œ**: ë³µì‚¬-ë¶™ì—¬ë„£ê¸°ë¡œ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥
- âœ… **ë¹„êµ ë¶„ì„**: CUDA/GPUì™€ì˜ ì°¨ì´ì  ëª…í™•íˆ ì„¤ëª…
- âœ… **ì‚°ì—… í˜„ì¥**: ì‹¤ì œ ë„ì… ì‹œ ê³ ë ¤ì‚¬í•­ê³¼ ì˜ì‚¬ê²°ì • ê°€ì´ë“œ
- âœ… **ìµœì‹  ì •ë³´**: 2025ë…„ ê¸°ì¤€ TPU v5e/v6e í¬í•¨

### í•µì‹¬ ë‚´ìš© ìš”ì•½

| ì£¼ì œ | í•µì‹¬ í¬ì¸íŠ¸ |
|------|-----------|
| **TPUë€?** | AI ì „ìš© ASIC, í–‰ë ¬ ì—°ì‚°ì— íŠ¹í™” (GPUì˜ 1/3 ë¹„ìš©) |
| **ì ‘ê·¼ ë°©ë²•** | Colab ë¬´ë£Œ, Kaggle ì£¼ 30ì‹œê°„, Google Cloud ìœ ë£Œ |
| **í”„ë ˆì„ì›Œí¬** | JAX (ìµœì ), PyTorch/XLA, TensorFlow |
| **ì €ìˆ˜ì¤€ í”„ë¡œê·¸ë˜ë°** | JAX Pallas (CUDA Triton ëŒ€ì²´) |
| **ì ìš© ì‚¬ë¡€** | ì¹´ì¹´ì˜¤ Kanna 1.5B, Google BERT, AlphaGo |
| **ì£¼ì˜ì‚¬í•­** | ì˜¨í”„ë ˆë¯¸ìŠ¤ ì–´ë ¤ì›€, ì¸ë ¥ êµìœ¡ í•„ìˆ˜, í•™ìŠµ ê³¡ì„  ìˆìŒ |

## ëª©ì°¨
1. [Google TPU ì†Œê°œ](#google-tpu-ì†Œê°œ)
2. [TPU í™˜ê²½ ì ‘ì† ë° ì„¤ì •](#tpu-í™˜ê²½-ì ‘ì†-ë°-ì„¤ì •)
3. [TPU ë””ë°”ì´ìŠ¤ ê°ì§€ ë° ìƒíƒœ í™•ì¸](#tpu-ë””ë°”ì´ìŠ¤-ê°ì§€-ë°-ìƒíƒœ-í™•ì¸)
4. [TPU ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§](#tpu-ì„±ëŠ¥-ëª¨ë‹ˆí„°ë§)
5. [ê°œë°œ í™˜ê²½ êµ¬ì¶•](#ê°œë°œ-í™˜ê²½-êµ¬ì¶•)
6. [JAX í”„ë ˆì„ì›Œí¬ í™œìš©](#jax-í”„ë ˆì„ì›Œí¬-í™œìš©)
7. [PyTorch/XLAë¡œ TPU ì‚¬ìš©í•˜ê¸°](#pytorchxlaë¡œ-tpu-ì‚¬ìš©í•˜ê¸°)
8. [TensorFlowë¡œ TPU ì‚¬ìš©í•˜ê¸°](#tensorflowë¡œ-tpu-ì‚¬ìš©í•˜ê¸°)
9. [ëª¨ë¸ í•™ìŠµ ë° ì¶”ë¡ ](#ëª¨ë¸-í•™ìŠµ-ë°-ì¶”ë¡ )
10. [ì„±ëŠ¥ ìµœì í™” íŒ](#ì„±ëŠ¥-ìµœì í™”-íŒ)
11. [CUDA ëŒ€ì²´ ê¸°ìˆ  ìŠ¤íƒ](#cuda-ëŒ€ì²´-ê¸°ìˆ -ìŠ¤íƒ)
12. [ì‹¤ì „ í”„ë¡œì íŠ¸ ì˜ˆì œ](#ì‹¤ì „-í”„ë¡œì íŠ¸-ì˜ˆì œ)
13. [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)
14. [TPU ë„ì… ì‹œ ì‹¤ì „ ê³ ë ¤ì‚¬í•­](#tpu-ë„ì…-ì‹œ-ì‹¤ì „-ê³ ë ¤ì‚¬í•­)
15. [ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (FAQ)](#ìì£¼-ë¬»ëŠ”-ì§ˆë¬¸-faq)
16. [ì¶”ê°€ í•™ìŠµ ìë£Œ](#ì¶”ê°€-í•™ìŠµ-ìë£Œ)

---

## Google TPU ì†Œê°œ

### TPUë€?
Google TPU (Tensor Processing Unit)ëŠ” êµ¬ê¸€ì´ ê°œë°œí•œ AI ì „ìš© ASIC(ì£¼ë¬¸í˜• ë°˜ë„ì²´)ì…ë‹ˆë‹¤. ë²”ìš© GPUì™€ ë‹¬ë¦¬ í–‰ë ¬ ì—°ì‚°ì— íŠ¹í™”ë˜ì–´ ìˆì–´ ë”¥ëŸ¬ë‹ í•™ìŠµ ë° ì¶”ë¡ ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤.

### TPU ì„¸ëŒ€ë³„ ì£¼ìš” ì‚¬ì–‘

#### TPU v5e (ìµœì‹  ì„¸ëŒ€)
- **ë©”ëª¨ë¦¬**: 16GB HBM (ì¹©ë‹¹)
- **ì•„í‚¤í…ì²˜**: Systolic Array (ë§¥ë™ ë°°ì—´)
- **ì—°ì‚° ìœ ë‹›**: MXU (Matrix Multiply Unit) + VPU (Vector Processing Unit)
- **íŠ¹ì§•**: ë¹„ìš© íš¨ìœ¨ì ì¸ í•™ìŠµ ë° ì¶”ë¡ 
- **ì‚¬ìš© í™˜ê²½**: Google Colab, Kaggle, Google Cloud

#### TPU v6e
- **ë©”ëª¨ë¦¬**: 31.25GB HBM (ì¹©ë‹¹)
- **ì„±ëŠ¥**: v5e ëŒ€ë¹„ ì•½ 2ë°° í–¥ìƒ
- **íŠ¹ì§•**: ëŒ€ê·œëª¨ ëª¨ë¸ í•™ìŠµì— ìµœì í™”

#### TPU v4
- **ë©”ëª¨ë¦¬**: 32GB HBM2 (ì¹©ë‹¹)
- **ì„±ëŠ¥**: 275 TFLOPS (BF16)
- **íŠ¹ì§•**: Pod êµ¬ì„±ìœ¼ë¡œ ìˆ˜ì²œ ê°œ ì¹© ì—°ê²° ê°€ëŠ¥

### ê°€ê²© ì •ë³´
- **Google Colab**: ë¬´ë£Œ (TPU v5e-1, ì œí•œì  ì‚¬ìš©)
- **Colab Pro**: ì›” $9.99 (ë” ë§ì€ TPU ì‹œê°„)
- **Kaggle**: ë¬´ë£Œ (ì£¼ë‹¹ 30ì‹œê°„ TPU ì‚¬ìš©)
- **Google Cloud TPU v5e**: ì‹œê°„ë‹¹ ì•½ $1.35 (ì¹©ë‹¹)
- **Google Cloud TPU v4**: ì‹œê°„ë‹¹ ì•½ $4.50 (ì¹©ë‹¹)

### TPU vs GPU ë¹„êµ

| íŠ¹ì§• | Google TPU | NVIDIA GPU |
|------|-----------|-----------|
| **ì•„í‚¤í…ì²˜** | Systolic Array | SIMT (Thread-based) |
| **ìµœì í™” ëŒ€ìƒ** | í–‰ë ¬ ì—°ì‚° (AI ì „ìš©) | ë²”ìš© ë³‘ë ¬ ì—°ì‚° |
| **ë©”ëª¨ë¦¬ êµ¬ì¡°** | HBM â†’ VMEM â†’ SMEM | HBM â†’ Shared Memory â†’ Registers |
| **í”„ë¡œê·¸ë˜ë°** | JAX, PyTorch/XLA, TensorFlow | CUDA, PyTorch, TensorFlow |
| **ë¹„ìš©** | ìƒëŒ€ì ìœ¼ë¡œ ì €ë ´ | ìƒëŒ€ì ìœ¼ë¡œ ê³ ê°€ |
| **ì ‘ê·¼ì„±** | í´ë¼ìš°ë“œ ì¤‘ì‹¬ | ì˜¨í”„ë ˆë¯¸ìŠ¤ + í´ë¼ìš°ë“œ |

### TPUì˜ ì‹¤ì œ í™œìš© ì‚¬ë¡€
- **ì¹´ì¹´ì˜¤ Kanna 1.5B**: TPUë¡œ í•™ìŠµëœ í•œêµ­ì–´ ì–¸ì–´ ëª¨ë¸
- **Google BERT**: TPU Podë¡œ í•™ìŠµ (ê¸°ì¡´ GPU ëŒ€ë¹„ í•™ìŠµ ì‹œê°„ ëŒ€í­ ë‹¨ì¶•)
- **AlphaGo**: TPU v1ìœ¼ë¡œ ì´ì„¸ëŒê³¼ì˜ ëŒ€êµ­ ìˆ˜í–‰
- **ëŒ€ê·œëª¨ ì´ë¯¸ì§€ ë¶„ë¥˜**: ImageNet í•™ìŠµ ì‹œê°„ ë‹¨ì¶•
- **ì—°êµ¬ í”„ë¡œì íŠ¸**: Colab/Kaggleì—ì„œ ë¬´ë£Œë¡œ ì‹¤í—˜ ê°€ëŠ¥

### TPUì˜ ì¥ì 
1. **ë¹„ìš© íš¨ìœ¨ì„±**: GPU ëŒ€ë¹„ ì €ë ´í•œ í´ë¼ìš°ë“œ ë¹„ìš©
2. **ë†’ì€ ì²˜ë¦¬ëŸ‰**: ëŒ€ê·œëª¨ ë°°ì¹˜ ì²˜ë¦¬ì— ìµœì í™”
3. **ë¬´ë£Œ ì ‘ê·¼**: Colab, Kaggleì—ì„œ ë¬´ë£Œ ì‚¬ìš© ê°€ëŠ¥
4. **í™•ì¥ì„±**: SuperPod êµ¬ì¡°ë¡œ ìˆ˜ì²œ ê°œ ì¹© ì—°ê²°
5. **ì „ë ¥ íš¨ìœ¨**: ì™€íŠ¸ë‹¹ ì„±ëŠ¥ì´ GPUë³´ë‹¤ ìš°ìˆ˜

### TPUì˜ ì œì•½ì‚¬í•­
1. **ë²”ìš©ì„± ë¶€ì¡±**: ê²Œì„, ê·¸ë˜í”½ ë Œë”ë§ ë¶ˆê°€
2. **ìƒíƒœê³„**: CUDA ëŒ€ë¹„ ì‘ì€ ê°œë°œì ì»¤ë®¤ë‹ˆí‹°
3. **í•™ìŠµ ê³¡ì„ **: JAX, XLA ë“± ìƒˆë¡œìš´ ë„êµ¬ í•™ìŠµ í•„ìš”
4. **ë””ë²„ê¹…**: GPU ëŒ€ë¹„ ë””ë²„ê¹… ë„êµ¬ ë¶€ì¡±
5. **ì˜¨í”„ë ˆë¯¸ìŠ¤ ì œí•œ**: ì£¼ë¡œ í´ë¼ìš°ë“œ í™˜ê²½ì—ì„œë§Œ ì‚¬ìš©

---

## TPU í™˜ê²½ ì ‘ì† ë° ì„¤ì •

### 1. Google Colabì—ì„œ TPU ì‚¬ìš©í•˜ê¸°

Google Colabì€ ë¬´ë£Œë¡œ TPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê°€ì¥ ì‰¬ìš´ ë°©ë²•ì…ë‹ˆë‹¤.

#### Colab TPU ì„¤ì • ë°©ë²•

1. **Google Colab ì ‘ì†**
   - https://colab.research.google.com ë°©ë¬¸
   - Google ê³„ì •ìœ¼ë¡œ ë¡œê·¸ì¸

2. **ìƒˆ ë…¸íŠ¸ë¶ ìƒì„±**
   - 'New notebook' í´ë¦­

3. **TPU ëŸ°íƒ€ì„ ì„¤ì •**
   - ìƒë‹¨ ë©”ë‰´: `ëŸ°íƒ€ì„` â†’ `ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½`
   - í•˜ë“œì›¨ì–´ ê°€ì†ê¸°: `TPU` ì„ íƒ
   - TPU ìœ í˜•: `TPU v5e-8` ë˜ëŠ” `TPU v5e-1` ì„ íƒ
   - `ì €ì¥` í´ë¦­

4. **TPU ì—°ê²° í™•ì¸**
   ```python
   import os
   
   # TPU ì£¼ì†Œ í™•ì¸
   if 'COLAB_TPU_ADDR' in os.environ:
       print(f"âœ“ TPU ì—°ê²°ë¨: {os.environ['COLAB_TPU_ADDR']}")
   else:
       print("âœ— TPUê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
   ```

### 2. Kaggleì—ì„œ TPU ì‚¬ìš©í•˜ê¸°

Kaggleì€ ì£¼ë‹¹ 30ì‹œê°„ì˜ ë¬´ë£Œ TPU ì‹œê°„ì„ ì œê³µí•©ë‹ˆë‹¤.

#### Kaggle TPU ì„¤ì • ë°©ë²•

1. **Kaggle ì ‘ì†**
   - https://www.kaggle.com ë°©ë¬¸
   - ê³„ì • ë¡œê·¸ì¸

2. **ìƒˆ ë…¸íŠ¸ë¶ ìƒì„±**
   - `Code` â†’ `New Notebook` í´ë¦­

3. **TPU í™œì„±í™”**
   - ìš°ì¸¡ íŒ¨ë„: `Settings` í´ë¦­
   - `Accelerator`: `TPU v3-8` ì„ íƒ
   - ë…¸íŠ¸ë¶ ìë™ ì¬ì‹œì‘

4. **TPU ì‚¬ìš© ì‹œê°„ í™•ì¸**
   - ìš°ì¸¡ ìƒë‹¨ì— ë‚¨ì€ TPU ì‹œê°„ í‘œì‹œ
   - ì£¼ë‹¹ 30ì‹œê°„ ì œí•œ

### 3. VSCodeì—ì„œ Colab TPU ì›ê²© ì—°ê²°

ë¡œì»¬ VSCodeì—ì„œ Colab TPU ì„œë²„ì— ì—°ê²°í•˜ì—¬ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì›¹ ë¸Œë¼ìš°ì € ì—†ì´ ë¡œì»¬ IDE í™˜ê²½ì—ì„œ TPUë¥¼ í™œìš©í•  ìˆ˜ ìˆì–´ ìƒì‚°ì„±ì´ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤.

#### VSCode Colab í™•ì¥ ì„¤ì¹˜

1. **í™•ì¥ í”„ë¡œê·¸ë¨ ì„¤ì¹˜**
   - VSCode ì‹¤í–‰
   - í™•ì¥ í”„ë¡œê·¸ë¨ ê²€ìƒ‰: `colab`
   - `Colab` í™•ì¥ ì„¤ì¹˜ (ì›€ì§ì´ëŠ” ì• ë‹ˆë©”ì´ì…˜ ê°€ì´ë“œ í¬í•¨)

2. **Colab ì—°ê²° ì„¤ì •**
   - ìƒˆ `.ipynb` íŒŒì¼ ìƒì„±
   - ìš°ì¸¡ ìƒë‹¨ `Select Kernel` í´ë¦­
   - `Colab` ì„ íƒ
   - `Autoconnect` ì„ íƒ

3. **Google ê³„ì • ì¸ì¦**
   - ë¸Œë¼ìš°ì €ì—ì„œ Google ë¡œê·¸ì¸ íŒì—… í‘œì‹œ
   - Colab ë¦¬ì†ŒìŠ¤ ì½ê¸°/ì“°ê¸°/ì‚­ì œ ê¶Œí•œ í—ˆìš©
   - ì¸ì¦ ì™„ë£Œ í›„ VSCodeë¡œ ìë™ ë³µê·€

4. **TPU ì„¤ì • ë° í™•ì¸**
   - Colab ì›¹ì—ì„œ ëŸ°íƒ€ì„ ìœ í˜•ì„ TPUë¡œ ë³€ê²½ (ë˜ëŠ” VSCodeì—ì„œ ì§ì ‘ ì„¤ì •)
   - VSCode í„°ë¯¸ë„ì—ì„œ `!pip install torch_xla[tpu]` ì‹¤í–‰
   - TPU ê°ì§€ ì½”ë“œ ì‹¤í–‰í•˜ì—¬ XLA ê°€ì†ê¸° í™•ì¸

**ì¥ì :**
- ë¡œì»¬ IDEì˜ ëª¨ë“  ê¸°ëŠ¥ í™œìš© (ì½”ë“œ ìë™ì™„ì„±, ë””ë²„ê¹… ë“±)
- ì›ê²© TPU ì„œë²„ì— íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ê´€ë¦¬ ê°€ëŠ¥
- ì›¹ ë¸Œë¼ìš°ì € íƒ­ ì „í™˜ ì—†ì´ ê°œë°œ ê°€ëŠ¥
- Git ì—°ë™ ë° ë²„ì „ ê´€ë¦¬ ìš©ì´

### 4. Google Cloud TPU VM ì ‘ì† (ê³ ê¸‰)

í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” Google Cloud TPU VMì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

```bash
# gcloud CLI ì„¤ì¹˜ (ë¡œì»¬)
curl https://sdk.cloud.google.com | bash
exec -l $SHELL

# í”„ë¡œì íŠ¸ ì„¤ì •
gcloud config set project YOUR_PROJECT_ID

# TPU VM ìƒì„±
gcloud compute tpus tpu-vm create tpu-vm-1 \
  --zone=us-central2-b \
  --accelerator-type=v5litepod-8 \
  --version=tpu-ubuntu2204-base

# SSH ì ‘ì†
gcloud compute tpus tpu-vm ssh tpu-vm-1 --zone=us-central2-b

# TPU VM ì‚­ì œ (ì‚¬ìš© í›„)
gcloud compute tpus tpu-vm delete tpu-vm-1 --zone=us-central2-b
```

---

## TPU ë””ë°”ì´ìŠ¤ ê°ì§€ ë° ìƒíƒœ í™•ì¸

### 1. PyTorch/XLAë¡œ TPU ê°ì§€í•˜ê¸°

PyTorchì—ì„œ TPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ `torch_xla` ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.

#### ì„¤ì¹˜

```bash
# PyTorch/XLA ì„¤ì¹˜
pip install torch torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html
```

#### TPU ê°ì§€ ì½”ë“œ

```python
import torch
import torch_xla
import torch_xla.core.xla_model as xm

# TPU ì¥ì¹˜ ê°€ì ¸ì˜¤ê¸°
device = xm.xla_device()
print(f"Device: {device}")

# XLA ì¥ì¹˜ í™•ì¸
if 'xla' in str(device):
    print("âœ“ TPU ë˜ëŠ” XLA ê°€ì†ê¸°ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ TPU ë””ë°”ì´ìŠ¤ ëª©ë¡
    tpu_devices = xm.get_xla_supported_devices('TPU')
    print(f"Available TPU devices: {tpu_devices}")
    
    # ë””ë°”ì´ìŠ¤ ê°œìˆ˜
    print(f"TPU ì½”ì–´ ê°œìˆ˜: {xm.xrt_world_size()}")
else:
    print("âœ— TPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
Device: xla:0
âœ“ TPU ë˜ëŠ” XLA ê°€ì†ê¸°ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.
Available TPU devices: ['TPU:0', 'TPU:1', 'TPU:2', 'TPU:3', 'TPU:4', 'TPU:5', 'TPU:6', 'TPU:7']
TPU ì½”ì–´ ê°œìˆ˜: 8
```

### 2. TensorFlowë¡œ TPU ê°ì§€í•˜ê¸°

TensorFlowëŠ” TPUë¥¼ ë„¤ì´í‹°ë¸Œë¡œ ì§€ì›í•©ë‹ˆë‹¤.

```python
import tensorflow as tf

print(f"TensorFlow ë²„ì „: {tf.__version__}")

# TPU ë””ë°”ì´ìŠ¤ í™•ì¸
tpu_devices = tf.config.list_logical_devices('TPU')

if len(tpu_devices) > 0:
    print(f"âœ“ {len(tpu_devices)}ê°œì˜ TPU ë””ë°”ì´ìŠ¤ ê°ì§€ë¨")
    for device in tpu_devices:
        print(f"  - {device.name}")
else:
    print("âœ— TPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
```

### 3. JAXë¡œ TPU ê°ì§€í•˜ê¸°

JAXëŠ” TPUë¥¼ ìœ„í•´ ì„¤ê³„ëœ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

```bash
# JAX TPU ë²„ì „ ì„¤ì¹˜
pip install "jax[tpu]" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html
```

```python
import jax
import jax.numpy as jnp

# TPU ë””ë°”ì´ìŠ¤ ê°œìˆ˜ í™•ì¸
device_count = jax.device_count()
print(f"âœ“ JAX ë””ë°”ì´ìŠ¤ ê°œìˆ˜: {device_count}")

# ë””ë°”ì´ìŠ¤ ëª©ë¡
devices = jax.devices()
print(f"ë””ë°”ì´ìŠ¤ ëª©ë¡:")
for i, device in enumerate(devices):
    print(f"  [{i}] {device}")

# ë””ë°”ì´ìŠ¤ íƒ€ì… í™•ì¸
device_type = devices[0].platform
print(f"ë””ë°”ì´ìŠ¤ íƒ€ì…: {device_type}")

# ê°„ë‹¨í•œ ì—°ì‚° í…ŒìŠ¤íŠ¸
x = jnp.ones((1000, 1000))
y = jnp.dot(x, x)
print(f"âœ“ TPU ì—°ì‚° í…ŒìŠ¤íŠ¸ ì„±ê³µ: {y.shape}")
```

---

## TPU ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### 1. tpu-info CLI ë„êµ¬

`tpu-info`ëŠ” TPU ë””ë°”ì´ìŠ¤ì˜ ì‹¤ì‹œê°„ ì„±ëŠ¥ ì§€í‘œë¥¼ ëª¨ë‹ˆí„°ë§í•˜ëŠ” CLI ë„êµ¬ì…ë‹ˆë‹¤.

#### ì„¤ì¹˜

```bash
# tpu-info ì„¤ì¹˜
pip install tpu-info -U
```

#### ê¸°ë³¸ ì‚¬ìš©ë²•

```bash
# TPU ìƒíƒœ í™•ì¸ (1íšŒì„±)
tpu-info

# ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ (2ì´ˆë§ˆë‹¤ ê°±ì‹ )
tpu-info --streaming --rate 2

# ë²„ì „ í™•ì¸
tpu-info --version

# í”„ë¡œì„¸ìŠ¤ ì •ë³´ í™•ì¸
tpu-info --process
```

#### HBM ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸

```bash
# HBM ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ë§Œ í™•ì¸
tpu-info --metric hbm_usage
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
TPU HBM Usage
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Chip   â”ƒ HBM Usage (GiB)                                               â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ 0      â”‚ 29.50 GiB / 31.25 GiB â”‚
â”‚ 1      â”‚ 21.50 GiB / 31.25 GiB â”‚
â”‚ 2      â”‚ 21.50 GiB / 31.25 GiB â”‚
â”‚ 3      â”‚ 21.50 GiB / 31.25 GiB â”‚
â”‚ 4      â”‚ 21.50 GiB / 31.25 GiB â”‚
â”‚ 5      â”‚ 21.50 GiB / 31.25 GiB â”‚
â”‚ 6      â”‚ 21.50 GiB / 31.25 GiB â”‚
â”‚ 7      â”‚ 21.50 GiB / 31.25 GiB â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Duty Cycle (ì‚¬ìš©ë¥ ) í™•ì¸

```bash
# Duty Cycleê³¼ HBM ì‚¬ìš©ëŸ‰ ë™ì‹œ í™•ì¸
tpu-info --metric duty_cycle_percent --metric hbm_usage
```

### 2. ì§€ì›ë˜ëŠ” ì§€í‘œ ëª©ë¡

```bash
# ëª¨ë“  ì§€ì› ì§€í‘œ í™•ì¸
tpu-info --list_metrics
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
â•­â”€ Supported Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚ buffer_transfer_latency
â”‚ collective_e2e_latency
â”‚ core_state
â”‚ device_to_host_transfer_latency
â”‚ duty_cycle_percent
â”‚ grpc_tcp_delivery_rate
â”‚ grpc_tcp_min_rtt
â”‚ hbm_usage
â”‚ host_to_device_transfer_latency
â”‚ queued_programs
â”‚ sequencer_state
â”‚ sequencer_state_detailed
â”‚ tensorcore_utilization
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

**ì£¼ìš” ì§€í‘œ ì„¤ëª…:**
- `hbm_usage`: HBM ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ê°€ì¥ ì¤‘ìš”!)
- `duty_cycle_percent`: TPU ì½”ì–´ ì‚¬ìš©ë¥  (0-100%)
- `tensorcore_utilization`: TensorCore í™œìš©ë„
- `buffer_transfer_latency`: ë²„í¼ ì „ì†¡ ì§€ì—° ì‹œê°„
- `grpc_tcp_min_rtt`: gRPC TCP ìµœì†Œ RTT
- `grpc_tcp_delivery_rate`: gRPC TCP ì „ì†¡ ì†ë„
- `collective_e2e_latency`: ì§‘í•© í†µì‹  ì§€ì—° ì‹œê°„
- `core_state`: ì½”ì–´ ìƒíƒœ ì •ë³´
- `sequencer_state`: ì‹œí€€ì„œ ìƒíƒœ (ìƒì„¸/ê°„ëµ)

---

## ê°œë°œ í™˜ê²½ êµ¬ì¶•

### 1. JAX ì„¤ì¹˜ (TPUìš©)

```bash
# JAX TPU ë²„ì „ ì„¤ì¹˜
pip install "jax[tpu]" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html

# ì„¤ì¹˜ í™•ì¸
python -c "import jax; print(f'JAX: {jax.__version__}'); print(f'Devices: {jax.devices()}')"
```

### 2. PyTorch/XLA ì„¤ì¹˜ (TPUìš©)

```bash
# PyTorch/XLA ì„¤ì¹˜
pip install torch torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html

# ì„¤ì¹˜ í™•ì¸
python -c "import torch; import torch_xla; import torch_xla.core.xla_model as xm; print(f'PyTorch: {torch.__version__}'); print(f'XLA Device: {xm.xla_device()}')"
```

### 3. TensorFlow ì„¤ì¹˜ (TPUìš©)

```bash
# TensorFlow 2.x (TPU ì§€ì›)
pip install tensorflow

# ì„¤ì¹˜ í™•ì¸
python -c "import tensorflow as tf; print(f'TensorFlow: {tf.__version__}'); print(f'TPU Available: {len(tf.config.list_logical_devices(\"TPU\"))}')"
```

---

## JAX í”„ë ˆì„ì›Œí¬ í™œìš©

JAXëŠ” Googleì´ ê°œë°œí•œ ê³ ì„±ëŠ¥ ìˆ˜ì¹˜ ê³„ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, TPUë¥¼ ìœ„í•´ ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

### 1. JAX ê¸°ë³¸ ì‚¬ìš©ë²•

#### ê°„ë‹¨í•œ ì—°ì‚°

```python
import jax
import jax.numpy as jnp

# NumPy ìŠ¤íƒ€ì¼ì˜ ë°°ì—´ ìƒì„±
x = jnp.array([1, 2, 3, 4, 5])
print(f"ë°°ì—´: {x}")

# í–‰ë ¬ ì—°ì‚°
A = jnp.ones((3, 3))
B = jnp.eye(3)
C = jnp.dot(A, B)
print(f"í–‰ë ¬ ê³±ì…ˆ ê²°ê³¼:\n{C}")

# ë””ë°”ì´ìŠ¤ í™•ì¸
print(f"ì—°ì‚° ë””ë°”ì´ìŠ¤: {x.device()}")
```

### 2. JAXì˜ í•µì‹¬ ê¸°ëŠ¥

#### JIT ì»´íŒŒì¼ (Just-In-Time)

```python
import jax
import jax.numpy as jnp
from jax import jit
import time

# ì¼ë°˜ í•¨ìˆ˜
def slow_function(x):
    return jnp.sum(x ** 2)

# JIT ì»´íŒŒì¼ëœ í•¨ìˆ˜
@jit
def fast_function(x):
    return jnp.sum(x ** 2)

# ì„±ëŠ¥ ë¹„êµ
x = jnp.ones((10000, 10000))

# ì²« ì‹¤í–‰ (ì»´íŒŒì¼ ì‹œê°„ í¬í•¨)
start = time.time()
result = fast_function(x)
result.block_until_ready()  # TPUëŠ” ë¹„ë™ê¸°ì´ë¯€ë¡œ ëŒ€ê¸° í•„ìš”
print(f"JIT ì²« ì‹¤í–‰: {time.time() - start:.4f}ì´ˆ")

# ë‘ ë²ˆì§¸ ì‹¤í–‰ (ì»´íŒŒì¼ ìºì‹œ ì‚¬ìš©)
start = time.time()
result = fast_function(x)
result.block_until_ready()
print(f"JIT ë‘ ë²ˆì§¸ ì‹¤í–‰: {time.time() - start:.4f}ì´ˆ")
```

#### ìë™ ë¯¸ë¶„ (Automatic Differentiation)

```python
import jax
import jax.numpy as jnp
from jax import grad

# í•¨ìˆ˜ ì •ì˜
def loss_function(x):
    return jnp.sum(x ** 2)

# ê·¸ë˜ë””ì–¸íŠ¸ í•¨ìˆ˜ ìƒì„±
grad_fn = grad(loss_function)

# ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
x = jnp.array([1.0, 2.0, 3.0])
gradient = grad_fn(x)
print(f"ì…ë ¥: {x}")
print(f"ê·¸ë˜ë””ì–¸íŠ¸: {gradient}")  # [2., 4., 6.]

# ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸
def multi_var_loss(params):
    w, b = params
    return w ** 2 + b ** 2

grad_multi = grad(multi_var_loss)
params = (3.0, 4.0)
print(f"ë‹¤ë³€ìˆ˜ ê·¸ë˜ë””ì–¸íŠ¸: {grad_multi(params)}")  # (6.0, 8.0)
```

#### ë²¡í„°í™” (Vectorization)

```python
import jax
import jax.numpy as jnp
from jax import vmap

# ë‹¨ì¼ ì…ë ¥ í•¨ìˆ˜
def single_input_fn(x):
    return jnp.sum(x ** 2)

# ë²¡í„°í™”ëœ í•¨ìˆ˜ (ë°°ì¹˜ ì²˜ë¦¬)
batched_fn = vmap(single_input_fn)

# ë°°ì¹˜ ë°ì´í„°
batch = jnp.array([[1, 2, 3],
                   [4, 5, 6],
                   [7, 8, 9]])

# ë°°ì¹˜ ì²˜ë¦¬
results = batched_fn(batch)
print(f"ë°°ì¹˜ ê²°ê³¼: {results}")  # [14, 77, 194]
```

### 3. JAXë¡œ ì‹ ê²½ë§ í•™ìŠµí•˜ê¸°

```python
import jax
import jax.numpy as jnp
from jax import grad, jit, vmap
from jax import random

# ê°„ë‹¨í•œ ì‹ ê²½ë§ ì •ì˜
def neural_network(params, x):
    w1, b1, w2, b2 = params
    hidden = jnp.tanh(jnp.dot(x, w1) + b1)
    output = jnp.dot(hidden, w2) + b2
    return output

# ì†ì‹¤ í•¨ìˆ˜
def loss_fn(params, x, y):
    predictions = neural_network(params, x)
    return jnp.mean((predictions - y) ** 2)

# íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
key = random.PRNGKey(0)
key, *subkeys = random.split(key, 5)

input_dim, hidden_dim, output_dim = 10, 20, 1
w1 = random.normal(subkeys[0], (input_dim, hidden_dim)) * 0.1
b1 = jnp.zeros(hidden_dim)
w2 = random.normal(subkeys[1], (hidden_dim, output_dim)) * 0.1
b2 = jnp.zeros(output_dim)

params = [w1, b1, w2, b2]

# ë”ë¯¸ ë°ì´í„°
X = random.normal(subkeys[2], (100, input_dim))
y = random.normal(subkeys[3], (100, output_dim))

# ê·¸ë˜ë””ì–¸íŠ¸ í•¨ìˆ˜
grad_fn = jit(grad(loss_fn))

# í•™ìŠµ ë£¨í”„
learning_rate = 0.01
for epoch in range(100):
    grads = grad_fn(params, X, y)
    params = [p - learning_rate * g for p, g in zip(params, grads)]
    
    if epoch % 20 == 0:
        current_loss = loss_fn(params, X, y)
        print(f"Epoch {epoch}, Loss: {current_loss:.4f}")
```

---

## PyTorch/XLAë¡œ TPU ì‚¬ìš©í•˜ê¸°

PyTorch/XLAëŠ” PyTorch ì½”ë“œë¥¼ TPUì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.

### 1. ê¸°ë³¸ ì‚¬ìš©ë²•

```python
import torch
import torch_xla
import torch_xla.core.xla_model as xm

# TPU ë””ë°”ì´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
device = xm.xla_device()
print(f"Using device: {device}")

# í…ì„œë¥¼ TPUë¡œ ì´ë™
x = torch.randn(3, 3).to(device)
y = torch.randn(3, 3).to(device)

# TPUì—ì„œ ì—°ì‚°
z = torch.matmul(x, y)

# ê²°ê³¼ë¥¼ CPUë¡œ ê°€ì ¸ì˜¤ê¸° (ì¶œë ¥ìš©)
print(f"Result:\n{z.cpu()}")
```

### 2. PyTorch/XLAë¡œ ëª¨ë¸ í•™ìŠµ

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch_xla
import torch_xla.core.xla_model as xm
import torch_xla.distributed.parallel_loader as pl

# TPU ë””ë°”ì´ìŠ¤
device = xm.xla_device()

# ê°„ë‹¨í•œ ëª¨ë¸ ì •ì˜
class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(784, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 10)
        self.relu = nn.ReLU()
        
    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# ëª¨ë¸ì„ TPUë¡œ ì´ë™
model = SimpleNet().to(device)

# ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# ë”ë¯¸ ë°ì´í„°
X_train = torch.randn(1000, 784)
y_train = torch.randint(0, 10, (1000,))

# í•™ìŠµ ë£¨í”„
model.train()
for epoch in range(10):
    # ë°ì´í„°ë¥¼ TPUë¡œ ì´ë™
    inputs = X_train.to(device)
    labels = y_train.to(device)
    
    # Forward pass
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    
    # Backward pass
    optimizer.zero_grad()
    loss.backward()
    
    # TPUì—ì„œ ì˜µí‹°ë§ˆì´ì € ìŠ¤í… ì‹¤í–‰
    xm.optimizer_step(optimizer)
    
    # ì£¼ê¸°ì ìœ¼ë¡œ ì†ì‹¤ ì¶œë ¥
    if epoch % 2 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item():.4f}")

print("âœ“ í•™ìŠµ ì™„ë£Œ")
```

### 3. ë©€í‹° ì½”ì–´ TPU í™œìš©

```python
import torch_xla.core.xla_model as xm
import torch_xla.distributed.xla_multiprocessing as xmp

def train_on_tpu(index):
    # ê° TPU ì½”ì–´ì—ì„œ ì‹¤í–‰ë  í•¨ìˆ˜
    device = xm.xla_device()
    
    # ëª¨ë¸ ìƒì„±
    model = SimpleNet().to(device)
    
    # í•™ìŠµ ì½”ë“œ...
    print(f"TPU ì½”ì–´ {index}ì—ì„œ í•™ìŠµ ì¤‘...")

# 8ê°œ TPU ì½”ì–´ì—ì„œ ë³‘ë ¬ ì‹¤í–‰
if __name__ == '__main__':
    xmp.spawn(train_on_tpu, nprocs=8)
```

---

## TensorFlowë¡œ TPU ì‚¬ìš©í•˜ê¸°

TensorFlowëŠ” TPUë¥¼ ë„¤ì´í‹°ë¸Œë¡œ ì§€ì›í•©ë‹ˆë‹¤.

### 1. TPU Strategy ì„¤ì •

```python
import tensorflow as tf

# TPU ì´ˆê¸°í™”
try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    print(f'âœ“ TPU ê°ì§€ë¨: {tpu.cluster_spec().as_dict()}')
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.TPUStrategy(tpu)
    print(f'âœ“ TPU ì½”ì–´ ê°œìˆ˜: {strategy.num_replicas_in_sync}')
except ValueError:
    print('âœ— TPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU/GPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.')
    strategy = tf.distribute.get_strategy()
```

### 2. TensorFlowë¡œ ëª¨ë¸ í•™ìŠµ

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# TPU Strategy ì„¤ì •
tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)
strategy = tf.distribute.TPUStrategy(tpu)

# Strategy ìŠ¤ì½”í”„ ë‚´ì—ì„œ ëª¨ë¸ ì •ì˜
with strategy.scope():
    model = keras.Sequential([
        layers.Dense(512, activation='relu', input_shape=(784,)),
        layers.Dropout(0.2),
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.2),
        layers.Dense(10, activation='softmax')
    ])
    
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

# ë”ë¯¸ ë°ì´í„°
import numpy as np
X_train = np.random.randn(1000, 784).astype(np.float32)
y_train = np.random.randint(0, 10, 1000)

# í•™ìŠµ
history = model.fit(
    X_train, y_train,
    batch_size=128,
    epochs=10,
    validation_split=0.2
)

print("âœ“ í•™ìŠµ ì™„ë£Œ")
```

---

## ëª¨ë¸ í•™ìŠµ ë° ì¶”ë¡ 

### 1. ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ (JAX)

```python
import jax
import jax.numpy as jnp
from jax import grad, jit, vmap
from jax import random
import numpy as np

# ê°„ë‹¨í•œ CNN ëª¨ë¸
def cnn_model(params, x):
    # ê°„ë‹¨í•œ 2ì¸µ ì‹ ê²½ë§ìœ¼ë¡œ ì‹œì—°
    w1, b1, w2, b2 = params
    x = x.reshape(x.shape[0], -1)  # Flatten
    hidden = jax.nn.relu(jnp.dot(x, w1) + b1)
    logits = jnp.dot(hidden, w2) + b2
    return logits

# ì†ì‹¤ í•¨ìˆ˜
def cross_entropy_loss(params, images, labels):
    logits = cnn_model(params, images)
    one_hot_labels = jax.nn.one_hot(labels, 10)
    return -jnp.mean(jnp.sum(one_hot_labels * jax.nn.log_softmax(logits), axis=1))

# ì •í™•ë„ ê³„ì‚°
def accuracy(params, images, labels):
    logits = cnn_model(params, images)
    predictions = jnp.argmax(logits, axis=1)
    return jnp.mean(predictions == labels)

# íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
key = random.PRNGKey(0)
key, *subkeys = random.split(key, 5)

input_dim = 28 * 28  # MNIST í¬ê¸°
hidden_dim = 512
output_dim = 10

w1 = random.normal(subkeys[0], (input_dim, hidden_dim)) * 0.1
b1 = jnp.zeros(hidden_dim)
w2 = random.normal(subkeys[1], (hidden_dim, output_dim)) * 0.1
b2 = jnp.zeros(output_dim)

params = [w1, b1, w2, b2]

# ë”ë¯¸ ë°ì´í„° (ì‹¤ì œë¡œëŠ” MNIST ë°ì´í„° ì‚¬ìš©)
images = random.normal(subkeys[2], (1000, 28, 28))
labels = random.randint(subkeys[3], (1000,), 0, 10)

# JIT ì»´íŒŒì¼ëœ í•™ìŠµ ìŠ¤í…
@jit
def train_step(params, images, labels, learning_rate):
    grads = grad(cross_entropy_loss)(params, images, labels)
    return [p - learning_rate * g for p, g in zip(params, grads)]

# í•™ìŠµ ë£¨í”„
learning_rate = 0.01
for epoch in range(20):
    params = train_step(params, images, labels, learning_rate)
    
    if epoch % 5 == 0:
        loss = cross_entropy_loss(params, images, labels)
        acc = accuracy(params, images, labels)
        print(f"Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {acc:.4f}")
```

### 2. ì „ì´ í•™ìŠµ (PyTorch/XLA)

```python
import torch
import torch.nn as nn
import torch_xla.core.xla_model as xm
from torchvision import models

# TPU ë””ë°”ì´ìŠ¤
device = xm.xla_device()

# ì‚¬ì „ í•™ìŠµëœ ResNet ëª¨ë¸ ë¡œë“œ
model = models.resnet18(pretrained=True)

# ë§ˆì§€ë§‰ ë ˆì´ì–´ êµì²´ (ì „ì´ í•™ìŠµ)
num_classes = 10
model.fc = nn.Linear(model.fc.in_features, num_classes)

# ëª¨ë¸ì„ TPUë¡œ ì´ë™
model = model.to(device)

# í•™ìŠµ ì„¤ì •
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

print("âœ“ ì „ì´ í•™ìŠµ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")
```

---

## ì„±ëŠ¥ ìµœì í™” íŒ

### 1. ë°°ì¹˜ í¬ê¸° ìµœì í™”

TPUëŠ” í° ë°°ì¹˜ í¬ê¸°ì—ì„œ ìµœê³  ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤.

```python
# JAXì—ì„œ ë°°ì¹˜ í¬ê¸° ìµœì í™”
batch_sizes = [32, 64, 128, 256, 512, 1024]

for batch_size in batch_sizes:
    # ë°°ì¹˜ ë°ì´í„° ìƒì„±
    batch_data = jnp.ones((batch_size, 784))
    
    # ì„±ëŠ¥ ì¸¡ì •
    import time
    start = time.time()
    result = model_fn(params, batch_data)
    result.block_until_ready()  # TPU ë™ê¸°í™”
    elapsed = time.time() - start
    
    print(f"Batch size {batch_size}: {elapsed:.4f}ì´ˆ")
```

**ê¶Œì¥ ë°°ì¹˜ í¬ê¸°:**
- TPU v5e: 128-512
- TPU v4: 256-1024
- ë©”ëª¨ë¦¬ê°€ í—ˆìš©í•˜ëŠ” í•œ í° ë°°ì¹˜ ì‚¬ìš©

### 2. JIT ì»´íŒŒì¼ í™œìš©

```python
# JAX JIT ì»´íŒŒì¼
from jax import jit

@jit
def optimized_function(x, y):
    return jnp.dot(x, y) + jnp.sum(x)

# PyTorch/XLAì—ì„œëŠ” xm.mark_step() ì‚¬ìš©
import torch_xla.core.xla_model as xm

for epoch in range(num_epochs):
    # í•™ìŠµ ì½”ë“œ...
    xm.mark_step()  # XLA ê·¸ë˜í”„ ì‹¤í–‰
```

### 3. ë©”ëª¨ë¦¬ ìµœì í™”

```python
# Gradient Checkpointing (ë©”ëª¨ë¦¬ ì ˆì•½)
import jax

def checkpoint_fn(x):
    # ì¤‘ê°„ ê²°ê³¼ë¥¼ ì €ì¥í•˜ì§€ ì•Šê³  ì¬ê³„ì‚°
    return jax.checkpoint(expensive_computation)(x)

# Mixed Precision (BF16 ì‚¬ìš©)
from jax import numpy as jnp

# BF16ìœ¼ë¡œ ì—°ì‚°
x_bf16 = x.astype(jnp.bfloat16)
result = model(x_bf16)
```

### 4. ë°ì´í„° íŒŒì´í”„ë¼ì¸ ìµœì í™”

```python
# TensorFlow Data Pipeline
import tensorflow as tf

# íš¨ìœ¨ì ì¸ ë°ì´í„° ë¡œë”©
dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
dataset = dataset.cache()  # ë©”ëª¨ë¦¬ì— ìºì‹œ
dataset = dataset.shuffle(buffer_size=10000)
dataset = dataset.batch(128)
dataset = dataset.prefetch(tf.data.AUTOTUNE)  # ë°±ê·¸ë¼ìš´ë“œ ë¡œë”©
```

---

## CUDA ëŒ€ì²´ ê¸°ìˆ  ìŠ¤íƒ

### TPU í™˜ê²½ì—ì„œ CUDAë¥¼ ëŒ€ì²´í•˜ëŠ” ê¸°ìˆ ë“¤

| NVIDIA GPU (CUDA) | Google TPU | ì„¤ëª… |
|-------------------|-----------|------|
| **CUDA C++** | **JAX Pallas** | ì €ìˆ˜ì¤€ ì»¤ë„ í”„ë¡œê·¸ë˜ë° |
| **cuDNN** | **XLA** | ë”¥ëŸ¬ë‹ ì—°ì‚° ìµœì í™” |
| **NCCL** | **gRPC/Collective Ops** | ë‹¤ì¤‘ ë””ë°”ì´ìŠ¤ í†µì‹  |
| **nvcc ì»´íŒŒì¼ëŸ¬** | **XLA ì»´íŒŒì¼ëŸ¬** | ì½”ë“œ ì»´íŒŒì¼ ë° ìµœì í™” |
| **Shared Memory** | **VMEM (Vector Memory)** | ì˜¨ì¹© ë©”ëª¨ë¦¬ |
| **Thread Blocks** | **Systolic Array** | ë³‘ë ¬ ì²˜ë¦¬ ì•„í‚¤í…ì²˜ |
| **OpenAI Triton** | **JAX Pallas** | ê³ ìˆ˜ì¤€ ì»¤ë„ ì‘ì„± |

### 1. JAX Pallas - TPUì˜ ì €ìˆ˜ì¤€ í”„ë¡œê·¸ë˜ë°

PallasëŠ” TPUì˜ ë©”ëª¨ë¦¬ì™€ ì—°ì‚° íŒŒì´í”„ë¼ì¸ì„ ì§ì ‘ ì œì–´í•  ìˆ˜ ìˆëŠ” ë„êµ¬ì…ë‹ˆë‹¤. NVIDIAì˜ CUDA C++ë‚˜ OpenAI Tritonê³¼ ìœ ì‚¬í•œ ì—­í• ì„ í•˜ë©°, TPUì˜ Systolic Array ì•„í‚¤í…ì²˜ë¥¼ ìµœëŒ€í•œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### Pallas í•µì‹¬ ê°œë…

**ë©”ëª¨ë¦¬ ê³„ì¸µ êµ¬ì¡°:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         HBM (High Bandwidth Memory) â”‚  â† ë©”ì¸ ë©”ëª¨ë¦¬ (16-32GB)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†• DMA Transfer
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    VMEM (Vector Memory) - On-chip  â”‚  â† ë²¡í„° ì—°ì‚°ìš© (ë¹ ë¦„)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    SMEM (Scalar Memory) - On-chip  â”‚  â† ìŠ¤ì¹¼ë¼ ì—°ì‚°ìš©
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MXU (Matrix Multiply Unit)         â”‚  â† í–‰ë ¬ ê³±ì…ˆ ì „ìš©
â”‚  VPU (Vector Processing Unit)       â”‚  â† ë²¡í„° ì—°ì‚°
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**CUDA vs Pallas ë¹„êµ:**

| NVIDIA GPU (CUDA) | Google TPU (Pallas) |
|-------------------|---------------------|
| Thread-based (SIMT) | Data-flow (Systolic Array) |
| HBM â†’ Shared Memory â†’ Registers | HBM â†’ VMEM â†’ SMEM |
| ìŠ¤ë ˆë“œ ì¸ë±ìŠ¤ ê³„ì‚° | BlockSpec & Grid ë§¤í•‘ |
| `__global__` ì»¤ë„ í•¨ìˆ˜ | `pallas_call` ë˜í¼ |

#### ê¸°ë³¸ Pallas ì»¤ë„ ì˜ˆì œ

```python
from jax.experimental import pallas as pl
import jax
import jax.numpy as jnp

# Pallas ì»¤ë„ ì •ì˜ - HBMê³¼ VMEM ê°„ ëª…ì‹œì  ë°ì´í„° ì´ë™
def matmul_kernel(x_ref, y_ref, z_ref):
    """
    x_ref, y_ref: ì…ë ¥ ë°ì´í„° ì°¸ì¡° (HBM)
    z_ref: ì¶œë ¥ ë°ì´í„° ì°¸ì¡° (HBM)
    """
    # HBMì—ì„œ VMEMìœ¼ë¡œ ë°ì´í„° ë¡œë“œ
    x = x_ref[...]
    y = y_ref[...]

    # í–‰ë ¬ ê³±ì…ˆ ìˆ˜í–‰ (MXU í™œìš©)
    z_ref[...] = jnp.dot(x, y)

# ì»¤ë„ ì‹¤í–‰ í•¨ìˆ˜
def pallas_matmul(x, y):
    return pl.pallas_call(
        matmul_kernel,
        out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), x.dtype)
    )(x, y)

# ì‚¬ìš© ì˜ˆì‹œ
x = jnp.ones((128, 128))
y = jnp.ones((128, 128))
result = pallas_matmul(x, y)
print(f"âœ“ Pallas ì»¤ë„ ì‹¤í–‰ ì™„ë£Œ: {result.shape}")
```

#### ê³ ê¸‰ Pallas: BlockSpecì„ ì‚¬ìš©í•œ íƒ€ì¼ë§

```python
from jax.experimental import pallas as pl
import jax.numpy as jnp

def tiled_matmul_kernel(x_ref, y_ref, z_ref):
    """íƒ€ì¼ë§ëœ í–‰ë ¬ ê³±ì…ˆ - ë©”ëª¨ë¦¬ íš¨ìœ¨ ìµœì í™”"""
    # ë¸”ë¡ ë‹¨ìœ„ë¡œ ë°ì´í„° ì²˜ë¦¬
    @pl.when(pl.program_id(0) < x_ref.shape[0])
    def body():
        x_block = x_ref[pl.program_id(0), :]
        y_block = y_ref[:, pl.program_id(1)]
        z_ref[pl.program_id(0), pl.program_id(1)] = jnp.dot(x_block, y_block)
    body()

# BlockSpecìœ¼ë¡œ íƒ€ì¼ í¬ê¸° ì •ì˜
def optimized_matmul(x, y, block_size=128):
    grid = (x.shape[0] // block_size, y.shape[1] // block_size)
    return pl.pallas_call(
        tiled_matmul_kernel,
        out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), x.dtype),
        grid=grid
    )(x, y)
```

#### Pallas íŒŒì´í”„ë¼ì´ë‹ ìµœì í™”

TPUëŠ” ë°ì´í„° ì´ë™ê³¼ ì—°ì‚°ì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰(Overlap)í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
def pipelined_kernel(x_ref, y_ref, z_ref):
    """DMA ì „ì†¡ê³¼ ì—°ì‚°ì„ ì˜¤ë²„ë©í•˜ëŠ” íŒŒì´í”„ë¼ì¸"""
    # ë‹¤ìŒ ë¸”ë¡ì„ ë¯¸ë¦¬ ë¡œë“œí•˜ë©´ì„œ í˜„ì¬ ë¸”ë¡ ì—°ì‚°
    for i in range(num_blocks):
        # Prefetch next block (ë¹„ë™ê¸° DMA)
        if i < num_blocks - 1:
            next_x = x_ref[i+1, ...]  # ë°±ê·¸ë¼ìš´ë“œ ë¡œë“œ

        # í˜„ì¬ ë¸”ë¡ ì—°ì‚°
        current_x = x_ref[i, ...]
        z_ref[i, ...] = process(current_x)
```

### 2. XLA (Accelerated Linear Algebra)

XLAëŠ” TPUì˜ í•µì‹¬ ì»´íŒŒì¼ëŸ¬ë¡œ, ê³ ìˆ˜ì¤€ ì—°ì‚°ì„ TPU ëª…ë ¹ì–´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

```python
import jax

# XLA ì»´íŒŒì¼ í™•ì¸
@jax.jit
def xla_optimized_fn(x):
    return jnp.sum(x ** 2) + jnp.mean(x)

# XLAê°€ ìë™ìœ¼ë¡œ ìµœì í™”
x = jnp.ones((1000, 1000))
result = xla_optimized_fn(x)
```

### 3. TPU ë©”ëª¨ë¦¬ ê³„ì¸µ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         HBM (High Bandwidth Memory) â”‚  â† ë©”ì¸ ë©”ëª¨ë¦¬ (16-32GB)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†• DMA Transfer
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    VMEM (Vector Memory) - On-chip   â”‚  â† ë²¡í„° ì—°ì‚°ìš© (ë¹ ë¦„)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    SMEM (Scalar Memory) - On-chip   â”‚  â† ìŠ¤ì¹¼ë¼ ì—°ì‚°ìš©
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MXU (Matrix Multiply Unit)         â”‚  â† í–‰ë ¬ ê³±ì…ˆ ì „ìš©
â”‚  VPU (Vector Processing Unit)       â”‚  â† ë²¡í„° ì—°ì‚°
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. GPU vs TPU í”„ë¡œê·¸ë˜ë° ë¹„êµ

#### CUDA (GPU)
```python
# CUDA ìŠ¤íƒ€ì¼ (ì˜ì‚¬ ì½”ë“œ)
__global__ void kernel(float* data) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    data[idx] = data[idx] * 2.0f;
}
```

#### JAX Pallas (TPU)
```python
# Pallas ìŠ¤íƒ€ì¼
def tpu_kernel(data_ref):
    data = data_ref[...]
    data_ref[...] = data * 2.0
```

---

## ì‹¤ì „ í”„ë¡œì íŠ¸ ì˜ˆì œ

### 1. MNIST ì†ê¸€ì”¨ ì¸ì‹ (JAX)

```python
import jax
import jax.numpy as jnp
from jax import grad, jit, vmap
from jax import random
import numpy as np

# MNIST ë°ì´í„° ë¡œë“œ (ì˜ˆì‹œ)
def load_mnist():
    # ì‹¤ì œë¡œëŠ” tensorflow_datasets ë“±ì„ ì‚¬ìš©
    # ì—¬ê¸°ì„œëŠ” ë”ë¯¸ ë°ì´í„° ìƒì„±
    key = random.PRNGKey(0)
    X_train = random.normal(key, (60000, 28, 28, 1))
    y_train = random.randint(key, (60000,), 0, 10)
    X_test = random.normal(key, (10000, 28, 28, 1))
    y_test = random.randint(key, (10000,), 0, 10)
    return X_train, y_train, X_test, y_test

# CNN ëª¨ë¸ ì •ì˜
def init_cnn_params(key):
    k1, k2, k3 = random.split(key, 3)
    
    # Conv1: 1 -> 32 channels
    conv1_w = random.normal(k1, (3, 3, 1, 32)) * 0.1
    conv1_b = jnp.zeros(32)
    
    # Conv2: 32 -> 64 channels
    conv2_w = random.normal(k2, (3, 3, 32, 64)) * 0.1
    conv2_b = jnp.zeros(64)
    
    # FC: 64*7*7 -> 10
    fc_w = random.normal(k3, (64 * 7 * 7, 10)) * 0.1
    fc_b = jnp.zeros(10)
    
    return [conv1_w, conv1_b, conv2_w, conv2_b, fc_w, fc_b]

def cnn_forward(params, x):
    conv1_w, conv1_b, conv2_w, conv2_b, fc_w, fc_b = params
    
    # Conv1 + ReLU + MaxPool
    x = jax.lax.conv(x, conv1_w, (1, 1), 'SAME')
    x = jax.nn.relu(x + conv1_b.reshape(1, 1, 1, -1))
    x = jax.lax.reduce_window(x, -jnp.inf, jax.lax.max, (1, 2, 2, 1), (1, 2, 2, 1), 'VALID')
    
    # Conv2 + ReLU + MaxPool
    x = jax.lax.conv(x, conv2_w, (1, 1), 'SAME')
    x = jax.nn.relu(x + conv2_b.reshape(1, 1, 1, -1))
    x = jax.lax.reduce_window(x, -jnp.inf, jax.lax.max, (1, 2, 2, 1), (1, 2, 2, 1), 'VALID')
    
    # Flatten + FC
    x = x.reshape(x.shape[0], -1)
    logits = jnp.dot(x, fc_w) + fc_b
    
    return logits

# ì†ì‹¤ í•¨ìˆ˜
@jit
def loss_fn(params, images, labels):
    logits = cnn_forward(params, images)
    one_hot = jax.nn.one_hot(labels, 10)
    return -jnp.mean(jnp.sum(one_hot * jax.nn.log_softmax(logits), axis=1))

# ì •í™•ë„
@jit
def accuracy(params, images, labels):
    logits = cnn_forward(params, images)
    return jnp.mean(jnp.argmax(logits, axis=1) == labels)

# í•™ìŠµ ìŠ¤í…
@jit
def train_step(params, images, labels, lr):
    grads = grad(loss_fn)(params, images, labels)
    return [p - lr * g for p, g in zip(params, grads)]

# ë©”ì¸ í•™ìŠµ ë£¨í”„
def train_mnist():
    # ë°ì´í„° ë¡œë“œ
    X_train, y_train, X_test, y_test = load_mnist()
    
    # íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
    key = random.PRNGKey(42)
    params = init_cnn_params(key)
    
    # í•™ìŠµ
    batch_size = 128
    num_epochs = 10
    learning_rate = 0.001
    
    for epoch in range(num_epochs):
        # ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ
        for i in range(0, len(X_train), batch_size):
            batch_x = X_train[i:i+batch_size]
            batch_y = y_train[i:i+batch_size]
            params = train_step(params, batch_x, batch_y, learning_rate)
        
        # ê²€ì¦
        train_acc = accuracy(params, X_train[:1000], y_train[:1000])
        test_acc = accuracy(params, X_test, y_test)
        print(f"Epoch {epoch+1}: Train Acc = {train_acc:.4f}, Test Acc = {test_acc:.4f}")
    
    return params

# ì‹¤í–‰
# params = train_mnist()
```

### 2. ì™„ì „í•œ TPU ê°ì§€ ë° ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸

```python
"""
Google Colab í™˜ê²½ì—ì„œ TPUë¥¼ ì™„ì „íˆ ê°ì§€í•˜ê³  í…ŒìŠ¤íŠ¸í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""
import os
import sys
import torch

def detect_and_test_tpu():
    """
    TPUë¥¼ ê°ì§€í•˜ê³  ê°„ë‹¨í•œ í…ì„œ ì—°ì‚°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
    """
    print(f"Python ë²„ì „: {sys.version}")
    print(f"PyTorch ë²„ì „: {torch.__version__}")

    # 1. í™˜ê²½ ë³€ìˆ˜ í™•ì¸ (Colab TPU ëŸ°íƒ€ì„)
    if 'COLAB_TPU_ADDR' not in os.environ:
        print("\n[!] ê²½ê³ : TPUê°€ ëŸ°íƒ€ì„ì— ì—°ê²°ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print("    ìƒë‹¨ ë©”ë‰´ 'ëŸ°íƒ€ì„' > 'ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½'ì—ì„œ 'TPU'ë¥¼ ì„ íƒí–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return False

    print(f"\nâœ“ TPU ì£¼ì†Œ ê°ì§€ë¨: {os.environ['COLAB_TPU_ADDR']}")

    # 2. torch_xla ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
    try:
        import torch_xla
        import torch_xla.core.xla_model as xm
    except ImportError:
        print("\n[!] ì˜¤ë¥˜: torch_xla ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("    ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
        print("    !pip install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html")
        return False

    # 3. TPU ì¥ì¹˜ ê°€ì ¸ì˜¤ê¸°
    try:
        device = xm.xla_device()
        print(f"\n[ì„±ê³µ] TPU ì¥ì¹˜ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤: {device}")

        # 4. í…ì„œ ì—°ì‚° í…ŒìŠ¤íŠ¸
        print("\n--- í…ì„œ ìƒì„± ë° ì—°ì‚° í…ŒìŠ¤íŠ¸ ---")

        # CPU í…ì„œ ìƒì„±
        t_cpu = torch.randn(2, 2)
        print(f"CPU í…ì„œ:\n{t_cpu}")
        print(f"ì¥ì¹˜: {t_cpu.device}")

        # TPUë¡œ í…ì„œ ì´ë™ ë° ì—°ì‚°
        t_tpu = t_cpu.to(device)
        result = t_tpu * 2

        # ê²°ê³¼ ë™ê¸°í™”
        xm.mark_step()

        print(f"\nTPU í…ì„œ (x2 ì—°ì‚° ê²°ê³¼):\n{result}")
        print(f"ìµœì¢… ì¥ì¹˜: {result.device}")
        print("\nâœ“ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

        return True

    except Exception as e:
        print(f"\n[!] TPU ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

# ì‹¤í–‰
if __name__ == "__main__":
    detect_and_test_tpu()
```

### 3. TPU HBM ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

```python
"""
tpu-infoë¥¼ ì‚¬ìš©í•œ ì‹¤ì‹œê°„ TPU ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸
"""
import subprocess
import time

def monitor_tpu_memory(duration_seconds=30, interval=2):
    """
    TPU HBM ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì£¼ê¸°ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤.

    Args:
        duration_seconds: ëª¨ë‹ˆí„°ë§ ì§€ì† ì‹œê°„ (ì´ˆ)
        interval: ì¸¡ì • ê°„ê²© (ì´ˆ)
    """
    print(f"TPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘ ({duration_seconds}ì´ˆ ë™ì•ˆ, {interval}ì´ˆ ê°„ê²©)")
    print("=" * 60)

    iterations = duration_seconds // interval

    for i in range(iterations):
        # tpu-info ëª…ë ¹ ì‹¤í–‰
        result = subprocess.run(
            ['tpu-info', '--metric', 'hbm_usage', '--metric', 'duty_cycle_percent'],
            capture_output=True,
            text=True
        )

        print(f"\n[ì¸¡ì • {i+1}/{iterations}] {time.strftime('%H:%M:%S')}")
        print(result.stdout)

        if i < iterations - 1:
            time.sleep(interval)

    print("=" * 60)
    print("ëª¨ë‹ˆí„°ë§ ì™„ë£Œ")

# ì‚¬ìš© ì˜ˆì‹œ
# monitor_tpu_memory(duration_seconds=60, interval=5)
```

### 4. ì´ë¯¸ì§€ ë¶„ë¥˜ (PyTorch/XLA)

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch_xla.core.xla_model as xm
from torchvision import datasets, transforms

# TPU ë””ë°”ì´ìŠ¤
device = xm.xla_device()

# ë°ì´í„° ì „ì²˜ë¦¬
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# CNN ëª¨ë¸
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.max_pool2d(x, 2)
        x = torch.relu(self.conv2(x))
        x = torch.max_pool2d(x, 2)
        x = torch.flatten(x, 1)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# ëª¨ë¸ ì´ˆê¸°í™”
model = SimpleCNN().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

print("âœ“ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")
```

---

## ë¬¸ì œ í•´ê²°

### 1. TPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ

**ì¦ìƒ:**
```
âœ— TPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
```

**í•´ê²° ë°©ë²•:**

1. **Colab/Kaggle ëŸ°íƒ€ì„ í™•ì¸**
   - Colab: `ëŸ°íƒ€ì„` â†’ `ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½` â†’ `TPU` ì„ íƒ
   - Kaggle: `Settings` â†’ `Accelerator` â†’ `TPU v3-8` ì„ íƒ

2. **í™˜ê²½ ë³€ìˆ˜ í™•ì¸**
   ```python
   import os
   print('COLAB_TPU_ADDR' in os.environ)  # Trueì—¬ì•¼ í•¨
   ```

3. **ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¬ì„¤ì¹˜**
   ```bash
   pip uninstall -y torch torch_xla
   pip install torch torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html
   ```

### 2. Out of Memory ì—ëŸ¬

**ì¦ìƒ:**
```
RuntimeError: TPU out of memory
```

**í•´ê²° ë°©ë²•:**

1. **ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°**
   ```python
   batch_size = 64  # 128ì—ì„œ 64ë¡œ ê°ì†Œ
   ```

2. **Gradient Checkpointing ì‚¬ìš©**
   ```python
   from jax import checkpoint
   
   @checkpoint
   def expensive_layer(x):
       return heavy_computation(x)
   ```

3. **Mixed Precision ì‚¬ìš©**
   ```python
   # BF16ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
   x = x.astype(jnp.bfloat16)
   ```

4. **ë©”ëª¨ë¦¬ ì •ë¦¬**
   ```python
   # JAX
   import jax
   jax.clear_backends()
   
   # PyTorch/XLA
   import torch_xla.core.xla_model as xm
   xm.mark_step()
   ```

### 3. í•™ìŠµ ì†ë„ê°€ ëŠë¦¼

**ì›ì¸ ë° í•´ê²°:**

1. **ì‘ì€ ë°°ì¹˜ í¬ê¸°**
   - TPUëŠ” í° ë°°ì¹˜ì—ì„œ ìµœì  ì„±ëŠ¥
   - ë°°ì¹˜ í¬ê¸°ë¥¼ 128 ì´ìƒìœ¼ë¡œ ì¦ê°€

2. **JIT ì»´íŒŒì¼ ë¯¸ì‚¬ìš©**
   ```python
   # JAX
   from jax import jit
   
   @jit
   def train_step(params, x, y):
       # í•™ìŠµ ì½”ë“œ
       pass
   ```

3. **ë°ì´í„° ë¡œë”© ë³‘ëª©**
   ```python
   # TensorFlow
   dataset = dataset.prefetch(tf.data.AUTOTUNE)
   dataset = dataset.cache()
   ```

4. **ë¶ˆí•„ìš”í•œ CPU-TPU ì „ì†¡**
   ```python
   # ë‚˜ìœ ì˜ˆ: ë§¤ ìŠ¤í…ë§ˆë‹¤ CPUë¡œ ì „ì†¡
   for step in range(1000):
       loss = train_step()
       print(loss.item())  # âœ— ëŠë¦¼
   
   # ì¢‹ì€ ì˜ˆ: ì£¼ê¸°ì ìœ¼ë¡œë§Œ ì „ì†¡
   for step in range(1000):
       loss = train_step()
       if step % 100 == 0:
           print(loss.item())  # âœ“ ë¹ ë¦„
   ```

### 4. libtpu ë²„ì „ ë¶ˆì¼ì¹˜

**ì¦ìƒ:**
```
libtpu version: N/A (incompatible environment)
```

**í•´ê²° ë°©ë²•:**

```bash
# Python ë²„ì „ í™•ì¸ (3.10 ê¶Œì¥)
python --version

# libtpu ì¬ì„¤ì¹˜
pip install --upgrade libtpu-nightly

# JAX ì¬ì„¤ì¹˜
pip install --upgrade "jax[tpu]" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html
```

### 5. XLA ì»´íŒŒì¼ ì‹œê°„ì´ ê¹€

**ì›ì¸:**
- ì²« ì‹¤í–‰ ì‹œ XLAê°€ ì½”ë“œë¥¼ ì»´íŒŒì¼í•˜ë¯€ë¡œ ì‹œê°„ì´ ê±¸ë¦¼

**í•´ê²°:**
```python
# ì›Œë°ì—… ì‹¤í–‰
dummy_input = jnp.ones((batch_size, input_dim))
_ = model(params, dummy_input)  # ì»´íŒŒì¼
_ = model(params, dummy_input).block_until_ready()  # ì™„ë£Œ ëŒ€ê¸°

# ì´í›„ ì‹¤í–‰ì€ ë¹ ë¦„
for epoch in range(num_epochs):
    result = model(params, real_input)  # ìºì‹œëœ ì»´íŒŒì¼ ì‚¬ìš©
```

---

## TPU ë„ì… ì‹œ ì‹¤ì „ ê³ ë ¤ì‚¬í•­

### 1. í•˜ë“œì›¨ì–´ ì¸í”„ë¼ ìš”êµ¬ì‚¬í•­

#### TPU SuperPod êµ¬ì¡°ì˜ íŠ¹ìˆ˜ì„±

TPUëŠ” ë‹¨ìˆœíˆ GPUë¥¼ TPUë¡œ êµì²´í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œëŠ” ë„ì…í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**TPU ë„¤íŠ¸ì›Œí‚¹ êµ¬ì¡°:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Lumentum OCS                         â”‚
â”‚              (Optical Circuit Switch)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†• â†• â†• â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TPU Chip â”‚â”€â”€â”‚ TPU Chip â”‚â”€â”€â”‚ TPU Chip â”‚â”€â”€â”‚ TPU Chip â”‚
â”‚ (Mesh)   â”‚  â”‚ (Mesh)   â”‚  â”‚ (Mesh)   â”‚  â”‚ (Mesh)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Direct Chip-to-Chip â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**GPU vs TPU ì¸í”„ë¼ ë¹„êµ:**

| êµ¬ë¶„ | NVIDIA GPU (DGX) | Google TPU (SuperPod) |
|------|------------------|----------------------|
| **ì¸í„°ì»¤ë„¥íŠ¸** | NVLink/NVSwitch (ì „ìš© ìŠ¤ìœ„ì¹˜) | OCS ê´‘ ìŠ¤ìœ„ì¹˜ (Direct Mesh) |
| **ë©”ì¸ë³´ë“œ** | DGX ì „ìš© ë©”ì¸ë³´ë“œ í•„ìš” | TPU Pod ì „ìš© ë™ ì‹œìŠ¤í…œ |
| **í™•ì¥ì„±** | ìˆ˜ì‹­~ìˆ˜ë°± ê°œ GPU | ìˆ˜ì²œ ê°œ TPU ì¹© |
| **êµì²´ ê°€ëŠ¥ì„±** | ë™ ë‹¨ìœ„ êµì²´ í•„ìš” | ë™ ì „ì²´ ì‹œìŠ¤í…œ êµì²´ í•„ìš” |
| **ì¡°ë¦½ ê°€ëŠ¥ì„±** | ë¶ˆê°€ëŠ¥ (DGX ì „ìš© ì„¤ê³„) | ë¶ˆê°€ëŠ¥ (í´ë¼ìš°ë“œ ì „ìš©) |

**í˜„ì‹¤ì ì¸ ì‹œì‚¬ì :**
- ê¸°ì¡´ GPU ë°ì´í„°ì„¼í„°ë¥¼ TPUë¡œ ì „í™˜í•˜ë ¤ë©´ **ë™ ì „ì²´ë¥¼ êµì²´**í•´ì•¼ í•¨
- ì˜¨í”„ë ˆë¯¸ìŠ¤ ë„ì…ì€ ë§¤ìš° ì œí•œì  (êµ¬ê¸€ í´ë¼ìš°ë“œ ì‚¬ìš© ê¶Œì¥)
- ì¼ë¡  ë¨¸ìŠ¤í¬ì˜ ì‚¬ë¡€: GPU 20ë§Œ ì¥ íˆ¬ì í›„ TPUë¡œ ì „í™˜ ë¶ˆê°€ (ì¸í”„ë¼ ì „ì²´ êµì²´ í•„ìš”)

### 2. ê°œë°œ ì¸ë ¥ í™•ë³´ì˜ í˜„ì‹¤

#### ìš”êµ¬ë˜ëŠ” ê¸°ìˆ  ìŠ¤íƒ

**GPU/CUDA ê°œë°œì â†’ TPU/JAX ì „í™˜ ì‹œ í•™ìŠµ í•„ìš” í•­ëª©:**

1. **í”„ë ˆì„ì›Œí¬ ì „í™˜**
   - PyTorch â†’ JAX ë˜ëŠ” PyTorch/XLA
   - CUDA ì»¤ë„ â†’ JAX Pallas ì»¤ë„
   - Triton â†’ Pallas

2. **ì•„í‚¤í…ì²˜ ì´í•´**
   - SIMT (Thread-based) â†’ Systolic Array (Data-flow)
   - ìŠ¤ë ˆë“œ ë¸”ë¡ ê°œë… â†’ BlockSpec & Grid ë§¤í•‘
   - Shared Memory â†’ VMEM/SMEM

3. **ë””ë²„ê¹… ë° ìµœì í™” ë„êµ¬**
   - NVIDIA Nsight â†’ tpu-info CLI
   - CUDA Profiler â†’ JAX Profiler
   - ì»¤ë®¤ë‹ˆí‹° ì§€ì› ë¶€ì¡± (CUDA ëŒ€ë¹„)

#### êµ­ë‚´ TPU ê°œë°œ ê²½í—˜ í˜„í™©

**ì‹¤ì œ TPU í”„ë¡œì íŠ¸ ì‚¬ë¡€:**
- **ì¹´ì¹´ì˜¤ Kanna 1.5B**: êµ­ë‚´ ìµœì´ˆ ëŒ€ê·œëª¨ TPU í•™ìŠµ í”„ë¡œì íŠ¸
- **ê²½í—˜ ë³´ìœ  ê°œë°œì**: ê·¹ì†Œìˆ˜ (ì „êµ­ ìˆ˜ì‹­ ëª… ìˆ˜ì¤€)
- **êµìœ¡ í”„ë¡œê·¸ë¨**: ê±°ì˜ ì „ë¬´

**ì¸ë ¥ í™•ë³´ ì „ëµ:**
```python
# í˜„ì‹¤ì ì¸ TPU ë„ì… ë¡œë“œë§µ
1. í´ë¼ìš°ë“œ TPUë¡œ ì†Œê·œëª¨ ì‹¤í—˜ (Colab, Kaggle)
2. í•µì‹¬ ê°œë°œì 2-3ëª… JAX êµìœ¡ (3-6ê°œì›”)
3. íŒŒì¼ëŸ¿ í”„ë¡œì íŠ¸ ì§„í–‰ (ì‘ì€ ëª¨ë¸ë¶€í„°)
4. ì ì§„ì  í™•ëŒ€ (ê¸°ì¡´ CUDA ì½”ë“œì™€ ë³‘í–‰)
```

### 3. ê²½ì˜ì§„ì´ í”íˆ ì˜¤í•´í•˜ëŠ” ë¶€ë¶„

#### ì˜¤í•´ 1: "ì„œë²„ì—ì„œ GPU ë¹¼ê³  TPUë§Œ ê½‚ìœ¼ë©´ ëœë‹¤"

**í˜„ì‹¤:**
- ë™ ì „ì²´ êµì²´ í•„ìš” (ìˆ˜ì–µ~ìˆ˜ì‹­ì–µ ì› íˆ¬ì)
- ë„¤íŠ¸ì›Œí¬ ì¸í”„ë¼ ì¬ì„¤ê³„ (OCS ê´‘ ìŠ¤ìœ„ì¹˜)
- ì „ë ¥ ë° ëƒ‰ê° ì‹œìŠ¤í…œ ì¬êµ¬ì„±

#### ì˜¤í•´ 2: "IT ë¶€ì„œ ì¸ë ¥ì´ë©´ ë‹¤ ê°œë°œ ê°€ëŠ¥í•˜ë‹¤"

**í˜„ì‹¤:**
- AI/ML ì—”ì§€ë‹ˆì–´ë„ ì „ë¬¸ ë¶„ì•¼ê°€ ë‹¤ë¦„
  - ì‹œìŠ¤í…œ ìš´ì˜ â‰  ëª¨ë°”ì¼ ì•± ê°œë°œ â‰  ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤
- TPU ê°œë°œì€ ê³ ë„ë¡œ ì „ë¬¸í™”ëœ ì˜ì—­
- ê²½í—˜ ìˆëŠ” ê°œë°œì í™•ë³´ê°€ í•µì‹¬

#### ì˜¤í•´ 3: "ë¹„ìš© ì ˆê° íš¨ê³¼ê°€ ì¦‰ì‹œ ë‚˜íƒ€ë‚œë‹¤"

**í˜„ì‹¤:**
```
ì´ˆê¸° íˆ¬ì ë¹„ìš©:
- ì¸ë ¥ êµìœ¡: 3-6ê°œì›” (ìƒì‚°ì„± 0)
- ì½”ë“œ ë§ˆì´ê·¸ë ˆì´ì…˜: 6-12ê°œì›”
- íŒŒì¼ëŸ¿ í”„ë¡œì íŠ¸: 3-6ê°œì›”
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ì´ 12-24ê°œì›” í›„ë¶€í„° ë¹„ìš© ì ˆê° íš¨ê³¼
```

### 4. ì„±ê³µì ì¸ TPU ë„ì… ì „ëµ

#### Phase 1: ê²€ì¦ ë‹¨ê³„ (1-3ê°œì›”)
```python
# Colab/Kaggle ë¬´ë£Œ TPUë¡œ POC
- ê¸°ì¡´ ëª¨ë¸ì„ JAXë¡œ ê°„ë‹¨íˆ í¬íŒ…
- ì„±ëŠ¥ ë° ë¹„ìš© ë¹„êµ ë¶„ì„
- ê°œë°œì í•™ìŠµ ê³¡ì„  í‰ê°€
```

#### Phase 2: íŒŒì¼ëŸ¿ ë‹¨ê³„ (3-6ê°œì›”)
```python
# Google Cloud TPU VMìœ¼ë¡œ ì‹¤ì „ í”„ë¡œì íŠ¸
- ì†Œê·œëª¨ í”„ë¡œë•ì…˜ ì›Œí¬ë¡œë“œ ì´ì „
- ëª¨ë‹ˆí„°ë§ ë° ìš´ì˜ í”„ë¡œì„¸ìŠ¤ í™•ë¦½
- íŒ€ êµìœ¡ ë° ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ì •ë¦½
```

#### Phase 3: í™•ëŒ€ ë‹¨ê³„ (6-12ê°œì›”)
```python
# GPUì™€ TPU í•˜ì´ë¸Œë¦¬ë“œ ìš´ì˜
- í•™ìŠµ: TPU (ëŒ€ê·œëª¨ ë°°ì¹˜, ë¹„ìš© íš¨ìœ¨)
- ì¶”ë¡ : GPU (ë‚®ì€ ì§€ì—°ì‹œê°„)
- ì ì§„ì  ì›Œí¬ë¡œë“œ ì´ì „
```

### 5. TPU vs GPU ì˜ì‚¬ê²°ì • ê°€ì´ë“œ

**TPUë¥¼ ì„ íƒí•´ì•¼ í•˜ëŠ” ê²½ìš°:**
- âœ“ ëŒ€ê·œëª¨ ëª¨ë¸ í•™ìŠµ (ìˆ˜ë°±ì–µ íŒŒë¼ë¯¸í„° ì´ìƒ)
- âœ“ ë°°ì¹˜ í¬ê¸°ê°€ í° ì‘ì—… (128 ì´ìƒ)
- âœ“ í´ë¼ìš°ë“œ í™˜ê²½ ì‚¬ìš©
- âœ“ ë¹„ìš© ì ˆê°ì´ ìµœìš°ì„  ëª©í‘œ
- âœ“ JAX ìƒíƒœê³„ ìˆ˜ìš© ê°€ëŠ¥

**GPUë¥¼ ìœ ì§€í•´ì•¼ í•˜ëŠ” ê²½ìš°:**
- âœ“ ì˜¨í”„ë ˆë¯¸ìŠ¤ ì¸í”„ë¼ í•„ìˆ˜
- âœ“ ë‚®ì€ ì§€ì—°ì‹œê°„ ì¶”ë¡  í•„ìš”
- âœ“ CUDA ì—ì½”ì‹œìŠ¤í…œ ì˜ì¡´ì„± ë†’ìŒ
- âœ“ ì†Œê·œëª¨ ë°°ì¹˜ ì‘ì—…
- âœ“ ë²”ìš© ì»´í“¨íŒ… í•„ìš” (ê²Œì„, ë Œë”ë§ ë“±)

---

## ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (FAQ)

### Q1: TPUì™€ GPU ì¤‘ ì–´ë–¤ ê²ƒì„ ì„ íƒí•´ì•¼ í•˜ë‚˜ìš”?

**A:** 
- **TPU ì„ íƒ**: ëŒ€ê·œëª¨ ë°°ì¹˜ í•™ìŠµ, ë¹„ìš© ì ˆê°, í´ë¼ìš°ë“œ í™˜ê²½
- **GPU ì„ íƒ**: ì‘ì€ ë°°ì¹˜, ë²”ìš© ì—°ì‚°, ì˜¨í”„ë ˆë¯¸ìŠ¤, CUDA ìƒíƒœê³„

### Q2: Colab ë¬´ë£Œ TPUì˜ ì œí•œì‚¬í•­ì€?

**A:**
- ì—°ì† ì‚¬ìš© ì‹œê°„ ì œí•œ (ì•½ 12ì‹œê°„)
- ìœ íœ´ ì‹œê°„ ì œí•œ (90ë¶„)
- ë™ì‹œ ì„¸ì…˜ ì œí•œ
- Colab Proë¡œ ì—…ê·¸ë ˆì´ë“œí•˜ë©´ ì œí•œ ì™„í™”

### Q3: PyTorch ì½”ë“œë¥¼ TPUì—ì„œ ì‹¤í–‰í•˜ë ¤ë©´?

**A:**
```python
# ìµœì†Œí•œì˜ ë³€ê²½ìœ¼ë¡œ TPU ì‚¬ìš©
import torch_xla.core.xla_model as xm

# device = torch.device('cuda')  # GPU
device = xm.xla_device()  # TPU

model = model.to(device)
# ë‚˜ë¨¸ì§€ ì½”ë“œëŠ” ë™ì¼
```

### Q4: JAXì™€ PyTorch ì¤‘ ì–´ë–¤ ê²ƒì„ ì‚¬ìš©í•´ì•¼ í•˜ë‚˜ìš”?

**A:**
- **JAX**: TPU ìµœì í™”, í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë°, ì—°êµ¬ìš©
- **PyTorch/XLA**: ê¸°ì¡´ PyTorch ì½”ë“œ ì¬ì‚¬ìš©, ìµìˆ™í•œ API

### Q5: TPUì—ì„œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‚˜ìš”?

**A:** ë„¤, ê°€ëŠ¥í•©ë‹ˆë‹¤.
```python
# PyTorch/XLA
from torchvision import models
model = models.resnet50(pretrained=True)
model = model.to(xm.xla_device())

# TensorFlow
model = tf.keras.applications.ResNet50(weights='imagenet')
```

### Q6: TPU ì‚¬ìš© ë¹„ìš©ì€ ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?

**A:**
- **ë¬´ë£Œ**: Colab (ì œí•œì ), Kaggle (ì£¼ 30ì‹œê°„)
- **ìœ ë£Œ**: 
  - Colab Pro: $9.99/ì›”
  - Google Cloud TPU v5e: ~$1.35/ì‹œê°„/ì¹©
  - Google Cloud TPU v4: ~$4.50/ì‹œê°„/ì¹©

### Q7: TPUì—ì„œ ë””ë²„ê¹…í•˜ëŠ” ë°©ë²•ì€?

**A:**
```python
# JAX ë””ë²„ê¹…
import jax
jax.config.update('jax_disable_jit', True)  # JIT ë¹„í™œì„±í™”

# ì¤‘ê°„ ê°’ ì¶œë ¥
def debug_fn(x):
    print(f"ì¤‘ê°„ ê°’: {x}")
    return x * 2

# PyTorch/XLA ë””ë²„ê¹…
import torch_xla.debug.metrics as met
print(met.metrics_report())  # ì„±ëŠ¥ ì§€í‘œ í™•ì¸
```

### Q8: ì—¬ëŸ¬ TPU ì½”ì–´ë¥¼ ë™ì‹œì— ì‚¬ìš©í•˜ë ¤ë©´?

**A:**
```python
# PyTorch/XLA
import torch_xla.distributed.xla_multiprocessing as xmp

def train_fn(index):
    device = xm.xla_device()
    # ê° ì½”ì–´ì—ì„œ ì‹¤í–‰ë  ì½”ë“œ
    
xmp.spawn(train_fn, nprocs=8)  # 8ê°œ ì½”ì–´ ì‚¬ìš©
```

---

## ì¶”ê°€ í•™ìŠµ ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [Google Cloud TPU Documentation](https://cloud.google.com/tpu/docs)
- [JAX Documentation](https://jax.readthedocs.io/)
- [PyTorch/XLA Documentation](https://pytorch.org/xla/)
- [TensorFlow TPU Guide](https://www.tensorflow.org/guide/tpu)

### ìœ ìš©í•œ ë„êµ¬
- [Google Colab](https://colab.research.google.com/)
- [Kaggle Notebooks](https://www.kaggle.com/code)
- [tpu-info CLI](https://github.com/google/cloud-accelerator-diagnostics/tree/main/tpu_info)

### ì»¤ë®¤ë‹ˆí‹°
- [JAX GitHub Discussions](https://github.com/google/jax/discussions)
- [PyTorch/XLA GitHub](https://github.com/pytorch/xla)
- [TensorFlow Forum](https://discuss.tensorflow.org/)

